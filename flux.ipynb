{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f28f7-f3d8-4a85-ad2f-1d99bddc64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import FluxPipeline, AutoencoderKL\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from transformers import T5EncoderModel, T5TokenizerFast, CLIPTokenizer, CLIPTextModel\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def bytes_to_giga_bytes(bytes):\n",
    "    return bytes / 1024 / 1024 / 1024\n",
    "\n",
    "def memory_opt(pipe):\n",
    "    pipe.enable_model_cpu_offload()\n",
    "    pipe.enable_sequential_cpu_offload()\n",
    "    try:\n",
    "        pipe.vae.enable_slicing()\n",
    "        pipe.vae.enable_tiling()\n",
    "    except:\n",
    "        pass\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71343abd-ea0a-4fa9-b57b-6dc38605e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def staged_inference(prompt, height=1024, width=1024, ckpt_id=\"black-forest-labs/FLUX.1-schnell\"):\n",
    "    flush()\n",
    "    \n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        ckpt_id, subfolder=\"text_encoder\", torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    text_encoder_2 = T5EncoderModel.from_pretrained(\n",
    "        ckpt_id, subfolder=\"text_encoder_2\", torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(ckpt_id, subfolder=\"tokenizer\")\n",
    "    tokenizer_2 = T5TokenizerFast.from_pretrained(ckpt_id, subfolder=\"tokenizer_2\")\n",
    "    pipeline = FluxPipeline.from_pretrained(\n",
    "        ckpt_id,\n",
    "        text_encoder=text_encoder,\n",
    "        text_encoder_2=text_encoder_2,\n",
    "        tokenizer=tokenizer,\n",
    "        tokenizer_2=tokenizer_2,\n",
    "        transformer=None,\n",
    "        vae=None,\n",
    "    ).to(\"cuda\")\n",
    "    pipeline = memory_opt(pipeline)\n",
    "    with torch.no_grad():\n",
    "        prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(\n",
    "            prompt=prompt, prompt_2=None, max_sequence_length=256\n",
    "        )\n",
    "    \n",
    "    del text_encoder\n",
    "    del text_encoder_2\n",
    "    del tokenizer\n",
    "    del tokenizer_2\n",
    "    del pipeline\n",
    "    \n",
    "    flush()\n",
    "    \n",
    "    pipeline = FluxPipeline.from_pretrained(\n",
    "        ckpt_id,\n",
    "        text_encoder=None,\n",
    "        text_encoder_2=None,\n",
    "        tokenizer=None,\n",
    "        tokenizer_2=None,\n",
    "        vae=None,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    ).to(\"cuda\")\n",
    "    pipeline = memory_opt(pipeline)\n",
    "    latents = pipeline(\n",
    "        prompt_embeds=prompt_embeds,\n",
    "        pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "        num_inference_steps=4,\n",
    "        guidance_scale=0.0,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        output_type=\"latent\",\n",
    "    ).images\n",
    "\n",
    "    del pipeline.transformer\n",
    "    del pipeline\n",
    "    \n",
    "    flush()\n",
    "    \n",
    "    vae = AutoencoderKL.from_pretrained(ckpt_id, revision=\"refs/pr/1\", subfolder=\"vae\", torch_dtype=torch.bfloat16)\n",
    "    vae = vae.to(\"cuda\")\n",
    "    vae_scale_factor = 2 ** (len(vae.config.block_out_channels))\n",
    "    image_processor = VaeImageProcessor(vae_scale_factor=vae_scale_factor)\n",
    "    with torch.no_grad():\n",
    "        latents = FluxPipeline._unpack_latents(latents, height, width, vae_scale_factor)\n",
    "        latents = (latents / vae.config.scaling_factor) + vae.config.shift_factor\n",
    "        image = vae.decode(latents, return_dict=False)[0]\n",
    "        image = image_processor.postprocess(image, output_type=\"pil\")\n",
    "    return image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e57e07-696d-423e-955f-af31a4d6238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ''\n",
    "\n",
    "i = 1\n",
    "image = staged_inference(p, height=512, width=512)\n",
    "image\n",
    "image.save(\"1.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cfb590-b5ea-4e21-9d49-401f4f56f061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
